**What is Big O ?**

The Big O notation, often denoted as O(n), is a mathematical notation used in computer science and
mathematics to describe the upper bound or worst-case time complexity of an algorithm or the growth
rate of a function. In simpler terms, it tells us how the runtime or resource usage of an algorithm
scales with the size of its input. When an algorithm is described as having a time complexity of O(n),
it means that the time it takes to execute the algorithm grows linearly with the size of the input.


 **What is linear complexity? O(n)**

  An algorithm with O(n) time complexity typically has a loop or iteration that processes each element of the input exactly once. The time it takes to process each element is constant, so the total time grows proportionally with the number of elements in the input.